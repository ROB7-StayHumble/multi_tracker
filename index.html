<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Multi tracker : Multiple object tracking for single camera in ROS">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Multi tracker</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/florisvb/multi_tracker">View on GitHub</a>

          <h1 id="project_title">Multi tracker</h1>
          <h2 id="project_tagline">Multiple object tracking for single camera in ROS</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/florisvb/multi_tracker/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/florisvb/multi_tracker/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="multi-tracker" class="anchor" href="#multi-tracker" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Multi tracker</h1>

<p>Multi tracker is a basic ROS package for real time tracking multiple objects in 2D. Primary testing has been on walking fruit flies. Only basic object-object interaction is supported by splitting objects that are larger than a specified size into two objects (thus three objects coming together will only be seen as 2 objects). Adjusting thresholds, and the image processing function can help improve robustness. System works reliably on a high end desktop from 2012 to track 10+ objects. The code supports multiple tracking instances on the computer through the "nodenum" option that is available on all the tracking nodes.</p>

<p>Below is a video that shows raw footage on the left, and the realtime output from the <code>liveviewer.py</code> node on the right.

<iframe width="560" height="315" src="https://www.youtube.com/embed/d2Zl3wrpyEg" frameborder="0" allowfullscreen></iframe>

<p>The package was built and tested with point grey usb firefly cameras on an Ubuntu (12.04) system, and Basler GigE cameras using the camera aravis driver on 12.04 and 14.04. However, there is no reason that it shouldn't work with other cameras.</p>

<h1>
<a id="installing" class="anchor" href="#installing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installing</h1>

<p>Install ROS, if you have not already done so (tested with ros hydro, full desktop install): <a href="http://wiki.ros.org/hydro/Installation/Ubuntu">http://wiki.ros.org/hydro/Installation/Ubuntu</a></p>

<p>Setup your catkin workspace, if you have not already done so: <a href="http://wiki.ros.org/catkin/Tutorials/create_a_workspace">http://wiki.ros.org/catkin/Tutorials/create_a_workspace</a></p>

<p>Then clone the git repository and run catkin make:</p>
<pre><code>$ cd ~/catkin_ws/src
$ git clone https://github.com/florisvb/multi_tracker.git
$ cd ~/catkin_ws
$ catkin_make</code></pre>


<h1>
<a id="cameras" class="anchor" href="#cameras" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cameras</h1>

<h4>Point grey usb camera</h4>

<p>Install the appropriate camera driver, such as: <a href="http://wiki.ros.org/pointgrey_camera_driver">http://wiki.ros.org/pointgrey_camera_driver</a>. To talk to the camera, you may need a udev rule. There is an example udev rule for a point grey firefly camera in the rules folder. Move this file to /etc/udev/rules.d directory.</p>

<h4>Basler GigE camera / Camera Aravis</h4>

<p>See aravis_install_notes</p>

<h1>
<a id="analysis" class="anchor" href="#analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Analysis</h1>

<p>To use the analysis tools: from inside multi_tracker, run python ./setup.py install. You may want to do this in a virtual environment: <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a></p>

<p>Analysis tools currently rely on pandas and hdf5 file formats.</p>

<h1>
<a id="overview" class="anchor" href="#overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h1>

<h2>
<a id="parameters" class="anchor" href="#parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

<p>(examples are found in the /demo folder)</p>

<p>camera_parameters.yaml: specifies key camera parameters, such as framerate, exposure time, etc. These parameter names may be camera brand dependent. The camera parameters can also be specified by running <em>rosrun rqt_reconfigure rqt_reconfigure</em> </p>

<p>tracker_parameters.yaml: specifies various tracking related parameters</p>

<p>data_association_parameters.yaml: specifies various data association related parameters</p>

<p>kalman_parameters.py: specifies the kalman parameters</p>

<h2>
<a id="nodes" class="anchor" href="#nodes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nodes</h2>

<h4>tracker_simplebuffer.py</h4>
<p>listens to <em>/camera/image_mono</em> (or whatever topic is specified in tracker_parameters.yaml), and uses the image processing routine specified in tracker_parameters.yaml to find the outer most contours of objects in the tracking space. Publishes a list of contours each frame, including x,y position, angle, and area. </p>

<h4>data_association.py</h4>
<p>listens to the <em>/multi_tracker/n/contours</em> topic and runs a simple kalman filter to do data association and filtering between subsequent frames. Publishes tracked objects to <em>/multi_tracker/n/tracked_objects</em></p>

<h4>save_data_to_hdf5.py</h4>
<p>listens to <em>/multi_tracker/n/tracked_objects</em> and saves the data to an h5 file. Alternatively, one can use "rosbag record" to record the data in ros format, which can replayed at a later time. Very rarely, not not never, does this node crash. For critical data, I recommend using rosbag record as a backup.</p>

<h4>liveviewer.py</h4>
<p>Listens to the camera topic specified in liveviewer_parameters.yaml, and overlays the tracked objects from the <em>/multi_tracker/n/tracked_objects</em>. Use the topic <em>/multi_tracker/n/processed_image</em> to see the tracking overlaid on the processed image from which the contours are calculated. This can help with debugging and optimizing the tracker parameters.</p>

<h4>delta_video_simplebuffer.py</h4>
<p>Listens to the camera topic specified in delta_video_parameters.yaml, and saves the pixels and values for any pixels that change more than the specified threshold. This results in a dramatically compressed filesize relative to a full resolution video. Artifacts are kept at an absolute minimum. Using a threshold of 10, and tracking 10 flies, 20 hours of 30 fps video comes to 20-40 GB.</p>

<h4>delta_video_player.py</h4>
<p>This node will replay a saved delta_video stream.</p>
 
<h4>save_bag.py</h4>
<p>Automatically starts and stops saving a rosbag file. Requires a config file. See raw_data_bag_config.py in the demo for a template.</p>

<h4>republish_pref_obj_data.py</h4>
<p>Republishes data from the oldest tracked object using a standard ROS message type (subject to change!). This is useful if doing a closed loop experiment with a single target.</p>


<h1>
<a id="running" class="anchor" href="#running" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running</h1>

<p>Minimal steps to run:</p>

<ol>
<li>copy the /demo folder to your home directory</li>
<li>get a camera running on the ROS network (see Cameras above), note the image topic it is publishing to</li>
<li>if the camera_image topic is not /camera/image_raw, edit the /demo/demo_1/src/tracker_parameters.yaml file so that /multi_tracker/1/tracker/image_topic matches the image_topic. For color cameras, it is best to use the camera/image_mono topic, if it exists.</li>
<li>from inside ~/demo/demo_1/src folder, run "roslaunch tracking_launcher.launch"
This will load all the yaml (parameter) files, and launch the tracker, data_association, save_hdf5_data, and liveviewer nodes.</li>
<li>Hit control-c to stop the node (and cease collecting data).</li>
</ol>

<p>Now you can try editing some of the contents of the yaml files to change the file structure and tracking parameters.</p>

<p>The two demo folders (demo_1 and demo_2) are there to illustrate how you can run two instances on one computer in parallel. You can even run them on the same camera feed, using ROI's to split a single camera feed into two experiments.</p>

<h1>
<a id="image-processing" class="anchor" href="#image-processing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Processing</h1>

<p>The code supports externally defined image processing functions, so you can write your own python image processing function, and specify the tracker to use that, rather than the default functions included in this package. To define your own image processing functions, set the parameter /multi_tracker/1/tracker/image_processing_module to be the exact path to your python file, and /multi_tracker/1/tracker/image_processor to the name of the function within that file.</p>

<p>To write your own image_processing function, look at the incredibly_basic function in image_processing.py, and start with this as a template. Don't forget to import some of the ROS specific stuff:</p>

<p>from multi_tracker.msg import Contourinfo, Contourlist
from multi_tracker.msg import Trackedobject, Trackedobjectlist
from multi_tracker.srv import resetBackgroundService</p>

<p>Alternatively, you can edit the image_processing.py file in multi_tracker/nodes, and add your function there. Then set /multi_tracker/1/tracker/image_processor to the name of your function.</p>

<h1>
<a id="lighting" class="anchor" href="#lighting" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lighting</h1>
<p>For our experiments, we typically use infrared lighting, which is invisible to human and most animal eyes, however, cameras (with the IR blocking filter removed) are exceptionally sensitive to this wavelength. This allows us to (a) image the animals independent of ambient lighting, and (b) independent of any visible light motion that may be used as an experimental cue. Infrared panels can be purchased, however, they are excessively expensive (~$800-1200). Below are instructions on how to make one for <$50.</p>
<img src="media/led_and_camera_view_600x450.jpg" alt="Lighting panel" style="width:600px;height:450px;">

Lighting parts list (note: parts ship from China, express shipping takes ~1-2 weeks to Los Angeles):
<ul>
  <li><a href="http://www.ledlightsworld.com/dc12v-smd3528300ir-infrared-850nm940nm-signle-chip-flexible-led-strips-60leds-48w-per-meter-p-1000598.html">850 nm LED reel</a></li>
  <li><a href="http://www.ledlightsworld.com/8mm-snap-down-strip-to-strip-with-wire-led-strip-connector-p-317.html">Connectors for LED strips (1 for each strip)</a></li>
  <li><a href="http://www.ledlightsworld.com/ul-certificated-led-power-supply-110220v-ac-to-12v-dc-p-1000344.html">Power supply (1-3 A depending on the total length</a></li>
</ul>


<p>Cut the reel at the designated cut points and attach the connectors to the appropriate +/- terminals to link multiple strips. In the example below, I have attached the strips to a sheet of laser cut acrylic, and use two additional pieces of acrylic to prevent the connector cables from moving.</p>

<img src="media/led_view_600x450.jpg" alt="Lighting panel 2" style="width:600px;height:450px;">

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Multi tracker maintained by <a href="https://github.com/florisvb">florisvb</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
