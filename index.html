<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Multi tracker : Multiple object tracking for single camera in ROS">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Multi tracker</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/florisvb/multi_tracker">View on GitHub</a>

          <h1 id="project_title">Multi tracker</h1>
          <h2 id="project_tagline">Multiple object tracking for single camera in ROS</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/florisvb/multi_tracker/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/florisvb/multi_tracker/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a id="multi-tracker" class="anchor" href="#multi-tracker" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Multi tracker</h1>

<p>Multi tracker is a basic ROS package for real time tracking multiple objects in 2D. Primary testing has been on walking fruit flies. Only basic object-object interaction is supported by splitting objects that are larger than a specified size into two objects (thus three objects coming together will only be seen as 2 objects). Adjusting thresholds, and the image processing function can help improve robustness. System works reliably on a high end desktop from 2012 to track 10+ objects. The code supports multiple tracking instances on the computer through the "nodenum" option that is available on all the tracking nodes.</p>

<p>Below is a video that shows raw footage on the left, and the realtime output from the <code>liveviewer.py</code> node on the right.

<iframe width="560" height="315" src="https://www.youtube.com/embed/d2Zl3wrpyEg" frameborder="0" allowfullscreen></iframe>

<p>The package was built and tested with point grey usb firefly cameras on an Ubuntu (12.04) system, and Basler GigE cameras using the camera aravis driver on 12.04 and 14.04. However, there is no reason that it shouldn't work with other cameras.</p>

<h1>
<a id="installing" class="anchor" href="#installing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installing</h1>

<p>Install ROS, if you have not already done so (tested with ros kinetic, full desktop install): <a href="http://wiki.ros.org/kinetic/Installation/Ubuntu">http://wiki.ros.org/kinetic/Installation/Ubuntu</a></p>

<p>Setup your catkin workspace, if you have not already done so: <a href="http://wiki.ros.org/catkin/Tutorials/create_a_workspace">http://wiki.ros.org/catkin/Tutorials/create_a_workspace</a></p>

<p>Then clone the git repository and run catkin make:</p>
<pre><code>$ cd ~/catkin_ws/src
$ git clone https://github.com/florisvb/multi_tracker.git
$ cd ~/catkin_ws
$ catkin_make</code></pre>


<h1>
<a id="cameras" class="anchor" href="#cameras" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Cameras</h1>

<h4>Point grey usb camera</h4>

<p>Install the appropriate camera driver, such as: <a href="http://wiki.ros.org/pointgrey_camera_driver">http://wiki.ros.org/pointgrey_camera_driver</a>. To talk to the camera, you may need a udev rule. There is an example udev rule for a point grey firefly camera in the rules folder. Move this file to /etc/udev/rules.d directory.</p>

<h4>Basler GigE camera / Camera Aravis</h4>

<p>See aravis_install_notes</p>

<h1>
<a id="analysis" class="anchor" href="#analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Analysis</h1>

<p>To use the analysis tools: from inside multi_tracker, run python ./setup.py install. You may want to do this in a virtual environment: <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a></p>
<p> You may need to install some of my other open source packages, e.g. <a href="https://github.com/florisvb/DataFit">DataFit</a>, and <a href="https://github.com/florisvb/FlyPlotLib">FlyPlotLib</a></p>
<p>Analysis tools currently rely on pandas and hdf5 file formats.</p>

<h1>
<a id="overview" class="anchor" href="#overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h1>

<h2>
<a id="parameters" class="anchor" href="#parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parameters</h2>

<p>(examples are found in the /demo folder)</p>

<p>camera_parameters.yaml: specifies key camera parameters, such as framerate, exposure time, etc. These parameter names may be camera brand dependent. The camera parameters can also be specified by running <em>rosrun rqt_reconfigure rqt_reconfigure</em> </p>

<p>tracker_parameters.yaml: specifies various tracking related parameters</p>

<p>data_association_parameters.yaml: specifies various data association related parameters</p>

<p>kalman_parameters.py: specifies the kalman parameters</p>

<h2>
<a id="nodes" class="anchor" href="#nodes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nodes</h2>

<h4>tracker_simplebuffer.py</h4>
<p>listens to <em>/camera/image_mono</em> (or whatever topic is specified in tracker_parameters.yaml), and uses the image processing routine specified in tracker_parameters.yaml to find the outer most contours of objects in the tracking space. Publishes a list of contours each frame, including x,y position, angle, and area. </p>

<h4>data_association.py</h4>
<p>listens to the <em>/multi_tracker/n/contours</em> topic and runs a simple kalman filter to do data association and filtering between subsequent frames. Publishes tracked objects to <em>/multi_tracker/n/tracked_objects</em></p>

<h4>save_data_to_hdf5.py</h4>
<p>listens to <em>/multi_tracker/n/tracked_objects</em> and saves the data to an h5 file. Alternatively, one can use "rosbag record" to record the data in ros format, which can replayed at a later time. Very rarely, not not never, does this node crash. For critical data, I recommend using rosbag record as a backup.</p>

<h4>liveviewer.py</h4>
<p>Listens to the camera topic specified in liveviewer_parameters.yaml, and overlays the tracked objects from the <em>/multi_tracker/n/tracked_objects</em>. Use the topic <em>/multi_tracker/n/processed_image</em> to see the tracking overlaid on the processed image from which the contours are calculated. This can help with debugging and optimizing the tracker parameters.</p>

<h4>delta_video_simplebuffer.py</h4>
<p>Listens to the camera topic specified in delta_video_parameters.yaml, and saves the pixels and values for any pixels that change more than the specified threshold. This results in a dramatically compressed filesize relative to a full resolution video. Artifacts are kept at an absolute minimum. Using a threshold of 10, and tracking 10 flies, 20 hours of 30 fps video comes to 20-40 GB.</p>

<h4>delta_video_player.py</h4>
<p>This node will replay a saved delta_video stream.</p>
 
<h4>save_bag.py</h4>
<p>Automatically starts and stops saving a rosbag file. Requires a config file. See raw_data_bag_config.py in the demo for a template.</p>

<h4>republish_pref_obj_data.py</h4>
<p>Republishes data from the oldest tracked object using a standard ROS message type (subject to change!). This is useful if doing a closed loop experiment with a single target.</p>


<h1>
<a id="running" class="anchor" href="#running" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running</h1>

<p>Minimal steps to run:</p>

<ol>
<li>copy the /demo folder to your home directory</li>
<li>get a camera running on the ROS network (see Cameras above), note the image topic it is publishing to</li>
<li>if the camera_image topic is not /camera/image_raw, edit the /demo/demo_1/src/tracker_parameters.yaml file so that /multi_tracker/1/tracker/image_topic matches the image_topic. For color cameras, it is best to use the camera/image_mono topic, if it exists.</li>
<li>from inside ~/demo/demo_1/src folder, run "roslaunch tracking_launcher.launch"
This will load all the yaml (parameter) files, and launch the tracker, data_association, save_hdf5_data, and liveviewer nodes.</li>
<li>Hit control-c to stop the node (and cease collecting data).</li>
</ol>

<p>Now you can try editing some of the contents of the yaml files to change the file structure and tracking parameters.</p>

<p>The two demo folders (demo_1 and demo_2) are there to illustrate how you can run two instances on one computer in parallel. You can even run them on the same camera feed, using ROI's to split a single camera feed into two experiments.</p>

<h1>
<a id="image-processing" class="anchor" href="#image-processing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Image Processing</h1>

<p>The code supports externally defined image processing functions, so you can write your own python image processing function, and specify the tracker to use that, rather than the default functions included in this package. To define your own image processing functions, set the parameter /multi_tracker/1/tracker/image_processing_module to be the exact path to your python file, and /multi_tracker/1/tracker/image_processor to the name of the function within that file.</p>

<p>To write your own image_processing function, look at the incredibly_basic function in image_processing.py, and start with this as a template. Don't forget to import some of the ROS specific stuff:</p>

<p>from multi_tracker.msg import Contourinfo, Contourlist
from multi_tracker.msg import Trackedobject, Trackedobjectlist
from multi_tracker.srv import resetBackgroundService</p>

<p>Alternatively, you can edit the image_processing.py file in multi_tracker/nodes, and add your function there. Then set /multi_tracker/1/tracker/image_processor to the name of your function.</p>

<h1>
<a id="lighting" class="anchor" href="#lighting" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lighting</h1>
<p>For our experiments, we typically use infrared lighting, which is invisible to human and most animal eyes, however, cameras (with the IR blocking filter removed) are exceptionally sensitive to this wavelength. This allows us to (a) image the animals independent of ambient lighting, and (b) independent of any visible light motion that may be used as an experimental cue. Infrared panels can be purchased, however, they are excessively expensive (~$800-1200). Below are instructions on how to make one for <$50.</p>
<img src="media/led_and_camera_view_600x450.jpg" alt="Lighting panel" style="width:600px;height:450px;">

<h4>Lighting parts list</h4>
<em>note: parts ship from China, express shipping takes ~1-2 weeks to Los Angeles</em>
<ul>
  <li><a href="http://www.ledlightsworld.com/dc12v-smd3528300ir-infrared-850nm940nm-signle-chip-flexible-led-strips-60leds-48w-per-meter-p-1000598.html">850 nm LED reel</a></li>
  <li><a href="http://www.ledlightsworld.com/8mm-snap-down-strip-to-strip-with-wire-led-strip-connector-p-317.html">Connectors for LED strips (1 for each strip)</a></li>
  <li><a href="http://www.ledlightsworld.com/ul-certificated-led-power-supply-110220v-ac-to-12v-dc-p-1000344.html">Power supply (1-3 A depending on the total length</a></li>
  <li><a href="http://www.ledlightsworld.com/dc-female-plug-cable-p-192.html">DC plug (LED strips ship with some, but not many)</a></li>
</ul>


<p>Cut the reel at the designated cut points and attach the connectors to the appropriate +/- terminals to link multiple strips. In the example below, I have attached the strips to a sheet of laser cut acrylic, and use two additional pieces of acrylic to prevent the connector cables from moving.</p>

<p>You can find simple solidworks templates for the base plate to which the LED's are stuck, as well as the arena walls, in the <a href=https://github.com/florisvb/multi_tracker/tree/master/solidworks/fly_arena_parts>solidworks folder</a>.</p>

<img src="media/led_view_600x450.jpg" alt="Lighting panel 2" style="width:600px;height:450px;">

<h3>Replaying Delta Video Movies</h3>

<p>Inside the <em>multi_tracker/examples/sample_data</em> you will find a delta_video bag file, along with its associated background image(s), and some launch files. The delta video bag file contains positions and values for each (x,y) pixel for each frame that are different from the background image by more than a threshold value (which was defined when the video was recorded). In cases where the background changes by a signficant amount (e.g. 20%), a new background image is saved.</p>

<p>To manually replay the delta video bag file as a standard ROS image topic, run the following commands, each in their own terminal window:</p>

<pre><code>roscore
rosparam set use_sim_time true
rosbag play [BAG FILENAME] --clock --pause
rosrun multi_tracker delta_video_player --in=[DELTA VIDEO TOPIC, e.g. /multi_tracker/1/delta_video] --out=[RECONSTRUCTED CAMERA TOPIC, e.g. /camera/image_raw] --directory=[DIRECTORY WHERE BACKGROUND IMAGES ARE FOUND - FULL PATH REQ'D]
rosrun image_view image_view image:=[RECONSTRUCTED CAMERA TOPIC, e.g. /camera/image_raw]
</code></pre>

<h5>Quick start example</h5>

<p>Inside <em>multi_tracker/examples/sample_data</em> edit <em>launch_delta_video.launch</em> such that the line defining the default path matches the actual <b>full</b> path where the sample data is located: <em><arg name="path" default="/home/caveman/catkin_ws/src/multi_tracker/examples/sample_data" /></em></p>
    
<p>Now launch the launch file using: <em>roslaunch launch_delta_video.launch</em></p>

<p>This will play the example bag file in paused mode; with the terminal window selected press space bar to start the movie.</p>

<h3>Tracking</h3>

<h5>Quick start example</h5>

<ol>
<li>copy the /examples/demo folder to your home directory as demo <br>(eg: <em>cp -r ~/catkin_ws/src/multi_tracker/examples/demo ~/</em>)</li>
<li>play the sample delta video according to instructions above</li>
<li>create the empty directory ~/demo/demo_1/data</li>
<li>from inside ~/demo/demo_1/src folder, run <em>roslaunch tracking_launcher.launch</em>
<br>This will load all the yaml (parameter) files, and launch the tracker, data_association, save_hdf5_data, delta_video, delta_video saving, and liveviewer nodes.</li>
<li>After the flies have all moved from their original position, with the liveviewer window selected, press 'a'. This will create a new background image, effectively erasing the flies starting positions from the background.</li>
<li>Hit control-c to stop the nodes (and cease collecting data).</li>
<li>Rename the data directory <br>(eg: <em>mv ~/demo/demo_1/data ~/demo/demo_1/test_data</em>)</i> 
</ol>

<h5>Use your own camera</h5>

<p>Set up your own camera to publish image topics to ROS. See, for example:</p>

<p><a href="http://wiki.ros.org/pointgrey_camera_driver">Point grey cameras</a>
<br><em>Notes: you may need to add a rule. For the firefly camera copy the rule multi_tracker/rules/99-point-grey-firefly.rules to /etc/udev/init.d, restart your computer, and replug in your camera. For other USB cameras, plug in the camera, run <i>dmesg</i> and note the idProduct and idVendor values for the camera, make the appropriate changes to multi_tracker/rules/99-point-grey-firefly.rules, and copy that new rule over to /etc/udev/init.d, restart, and replug your camera.</em><p>

<p><a href="https://github.com/florisvb/multi_tracker/blob/master/aravis_install_notes/aravis_install_notes.md">GigE cameras, e.g. Basler Ace</a></p>

<p>Check the image topic that your camera publishes to using <em>rostopic list</em>. If a image_mono topic is available, use that one. Edit the tracker_parameters.yaml file in ~/demo/demo_1/src to match that image topic. Now, with the camera topic running, and an empty data directory in ~/demo/demo_1 launch the tracking_launcher.launch file.</p>

<h3>Data Analysis</h3>

<p>Follow the instructions from the tracking quick start example (using the sample delta video, for example). Now you should have a data directory in ~/demo/demo_1/test_data with an hdf5 file (your tracked data), delta_video bag file, and background images for the delta video and tracking. Copy the configuration file multi_tracker/examples/sample_data/config_20160412_134708_N1.py to your data directory. Rename the file such that the data-time stamp matches the prefix of the hdf5 file.</p>

<p>Open the jupyter notebook, update the data path, and run each segment for simple examples: multi_tracker/examples/data_processing/example_data_processing.ipynb</p>

<h3>Data visualization and editing GUI</h3>

<p>Follow the Data Analysis steps. Then, from multi_tracker/multi_tracker_analysis, run: <em>python ./trajectory_viewer_gui_v2.py --path=[DATA PATH]</em></p>

<p>The bottom panel shows the time history of the number of trajectories. Drag / resize the light blue box to select a region of time. Trajectories in that time region are displayed in the top panel. Move / zoom / change aspect of the top panel using the left mouse / scroll wheel / right mouse. 

<li>Load / Play movie: loads delta video bag and plays back the movie. May produce erratic behavior in the case of the included example because of colliding delta video topics. In original data it works fine.</li>
<li>Join Trajectories: Click "select trajecs" and select 2 or more trajectories (id's appear in the box, and selected trajectory is bold). Click "join trajecs". Instructions are saved to an instruction file - original data is never modified.</li>
<li>Cut Trajectories: Click "cut trajecs" and click on a a trajectory where you wish to clip it. The trajectory will get cut into 2.</li>
<li>Delete Trajectories: Click "delete trajecs" and click on a trajectory to delete it. Or: click "select trajecs" and select a trajectory, then click "delete trajecs".</li>
<li>Erratic behavior: click "clear selection / data"</li>
<li>Annotations: select a trajectory, add a text annotation to the box, click the check box, and click "save annotation". This saves the annotation to a pickle file for use in analysis.</li>

<h3>Helpful tips</h3>

      <h5>I already have a movie, how can I track it?</h5>
      Use the ROS package <a href="http://wiki.ros.org/video_stream_opencv">video_stream_opencv</a>.
      <h5>My movie has one object, and it moves very fast, why can't I track it well?</h5>
      Try increasing the following parameter in the data_association_parameters.yaml file: 
      <code>/multi_tracker/1/data_association/n_covariances_to_reject_data: 3</code>
      This number is multiplied by the covariance of each tracked object to give an acceptance radius. Any objects in the next frame that lie within this radius are considered viable. The nearest of these objects will be chosen for the update step. Larger values mean more continuous tracking, but a higher likelihood for eroneous tracking if there are multiple objects or noise. To smooth the tracking, adjust the coefficients for R and Q in the kalman parameters. 
      <h5>My movie is playing, but the liveviewer does not work.</h5> 
      Try changing the camera_encoding parameters in the liveviewer_parameters.yaml to match your camera (e.g. switch from mono8 to rgb).
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Multi tracker maintained by <a href="https://github.com/florisvb">florisvb</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
